---
title: "Midterm"
author: "Mohan Rajendran"
date: '`r Sys.Date()`'
output:
  word_document:
    fig_height: 4
    fig_width: 4.5
  pdf_document:
    fig_height: 4
    fig_width: 4.5
  html_document:
    fig_height: 4
    fig_width: 4.5
---


```{r, setup, include=FALSE}
require(mosaic)   # Load additional packages here 

# Some customization.  You can alter or delete as desired (if you know what you are doing).
trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

Read the data and convert variables to factors. Rename variables so they convey the meaning properly
```{r}
## Read the data and convert variables to factors
input = read.csv("birthwt.csv")
names(input)
input <- subset(input, select = -c(bwt) )
sapply(input, class)
input <- with(input, {
  race <- factor(race, labels = c("white", "black", "other"))
  ptl <- factor(ptl)
  ftv <- factor(ftv)
  
  data.frame(low = factor(low), age, last_menstral_weight = lwt, race, smoke = factor(smoke),
             preterm_labours = ptl, hyper_tension = factor(ht), uterine_irritability = factor(ui), physician_visits = ftv)
})
birthwt = input
##Remove NA values
birthwt <- na.omit(birthwt)

```

```{r}
# read in libraries
library(MASS)
library(stats)
library(randomForest)

```

Plot histogram for the numerical variables such as age, last_menstral_weight. The plots appear normally distributed with slight skewness which is normal for age and weight related variables. Coleniarity is visually examined using pairs plot and found to be not a major concern.
```{r}

hist(birthwt$age)
hist(birthwt$last_menstral_weight)
table(birthwt$low)
pairs(birthwt)

```
Since most of the variables are factors and not continuous variables we can eliminated LDA and QDA classification approaches. So considering there is no collinearity and normal distribution of numerical variables, it is decided to perform Logistical model and Random Forest tree classification.
The full model is plotted and VIF is computed. There is no evidence of collinearity based on VIF. Backward selection step process is conducted and non significant variables are removed based on p value which resulted in following variable selection last_menstral_weight + race + smoke + preterm_labours + hyper_tension. For Random Forest classification mtry paramter will be set to 5
```{r}
fullModel <- glm(low ~ ., data = birthwt, family = 'binomial')
summary(fullModel)
car::vif(fullModel)

backwards = step(fullModel) 

```

```{r}
stepModel <- glm(low ~ last_menstral_weight + race + smoke + preterm_labours + 
    hyper_tension, data = birthwt, family = 'binomial')
summary(stepModel)
car::vif(stepModel)

```
We will perform the model assesment using 10 fold CV methodology. 
```{r warning=FALSE}
##### model assessment OUTER 10-fold CV (with model selection INNER 10-fold CV as part of model-fitting) #####

xy.out = birthwt
n.out = dim(xy.out)[1]

#define the cross-validation splits 
k.out = 10 
groups.out = c(rep(1:k.out,floor(n.out/k.out)),(1:(n.out-k.out*floor(n.out/k.out))))  #produces list of group labels
set.seed(5)
cvgroups.out = sample(groups.out,n.out)  #orders randomly, with seed (8) 

allpredictedCV.out = rep(NA,n.out)

##### model assessment OUTER shell #####
for (j in 1:k.out)  {  #be careful not to re-use loop indices
  groupj.out = (cvgroups.out == j)

  # define the training set for outer loop
  trainxy.out = xy.out[!groupj.out,]
  
  #define the validation set for outer loop
  testxy.out = xy.out[groupj.out,]

  ##############################################
  ###   model selection on trainxy.out       ###
  ##############################################
  ##entire model-fitting process##
  xy.in = trainxy.out  # fixed to be fit ONLY to the training data from the outer split
  n.in = dim(xy.in)[1]
  ncv = 10
  
  x.in = model.matrix(low~.,data=xy.in)[,-1]
  y.in = xy.in[,1]

  if ((n.in%%ncv) == 0) {
    groups.in= rep(1:ncv,floor(n.in/ncv))} else {
      groups.in=c(rep(1:ncv,floor(n.in/ncv)),(1:(n.in%%ncv)))
    }
  cvgroups.in = sample(groups.in,n.in)
  
  # set up storage
  allpredictedcv10 = matrix(,ncol=2,nrow=n.in)
  
  # with model selection 
  for (i in 1:ncv) {
    newdata.in = xy.in[cvgroups.in==i,]
    newX <- model.matrix(low~.,data=newdata.in)[,-1]

    log2fit = glm(low ~ last_menstral_weight + race + smoke + preterm_labours + 
    hyper_tension , data=xy.in, subset=(cvgroups.in!=i), family=binomial)
    log2fit$xlevels[["physician_visits"]] <- levels(birthwt$physician_visits)
    log2fit$xlevels[["race"]] <- levels(birthwt$race)
    log2fit$xlevels[["smoke"]] <- levels(birthwt$smoke)
    log2fit$xlevels[["preterm_labours"]] <- levels(birthwt$preterm_labours )
    log2fit$xlevels[["hyper_tension"]] <- levels(birthwt$hyper_tension)
    log2fit$xlevels[["uterine_irritability"]] <- levels(birthwt$uterine_irritability)
    log2prob = predict(log2fit,newdata.in,type="response")
    log2fact = rep(1,dim(newdata.in)[1]); 
    log2fact[log2prob > 0.5] = 2
    allpredictedcv10[cvgroups.in==i,1] = log2fact
    
    RFcvfit = randomForest(low~., data = xy.in[cvgroups.in!=i,], mtry = 5, importance = T)
    RFcvfit$xlevels[["physician_visits"]] <- levels(birthwt$physician_visits)
    RFcvfit$xlevels[["race"]] <- levels(birthwt$race)
    RFcvfit$xlevels[["smoke"]] <- levels(birthwt$smoke)
    RFcvfit$xlevels[["preterm_labours"]] <- levels(birthwt$preterm_labours )
    RFcvfit$xlevels[["hyper_tension"]] <- levels(birthwt$hyper_tension)
    RFcvfit$xlevels[["uterine_irritability"]] <- levels(birthwt$uterine_irritability)
    RFprob = predict(RFcvfit,newdata.in,type="response")
    RFfact = rep(1,dim(newdata.in)[1]); 
    RFfact[RFprob > 0.5] = 2
    allpredictedcv10[cvgroups.in==i,2] = RFfact
  }   

  #relabel as original values, not factor levels
  allpredictedcv10 = allpredictedcv10-1  # now a table of predicted 0-1 values for HD
  
  #compute the CV values
  allcv10 = rep(0,2)
  for (m in 1:2) allcv10[m] = sum(xy.in$low!=allpredictedcv10[,m])/n.in
  bestmodels = (1:2)[allcv10 == min(allcv10)]
  bestmodels

##############################################
  ###   resulting in bestmodels              ###
  ##############################################

  bestmodel = ifelse(length(bestmodels)==1,bestmodels,sample(bestmodels,1))
  print(allcv10)
  print(paste("Best model at outer loop",j,"is",bestmodel))

  if (bestmodel == 1)  {
    log2fit.train = glm(low ~ last_menstral_weight + race + smoke + preterm_labours + 
    hyper_tension , data= trainxy.out, family=binomial)
    log2fit.train$xlevels[["physician_visits"]] <- levels(birthwt$physician_visits)
    log2fit.train$xlevels[["race"]] <- levels(birthwt$race)
    log2fit.train$xlevels[["smoke"]] <- levels(birthwt$smoke)
    log2fit.train$xlevels[["preterm_labours"]] <- levels(birthwt$preterm_labours )
    log2fit.train$xlevels[["hyper_tension"]] <- levels(birthwt$hyper_tension)
    log2fit.train$xlevels[["uterine_irritability"]] <- levels(birthwt$uterine_irritability)
    log2prob.test = predict(log2fit.train,testxy.out,type="response")
    predictvalid = rep(1,dim(testxy.out)[1]); 
    predictvalid[log2prob.test > 0.5] = 2
  }
  if (bestmodel == 2)  {
    RFfit.train = randomForest(as.factor(low)~., data = xy.in[cvgroups.in!=i,], mtry = 5, importance = T)
    RFfit.train$xlevels[["physician_visits"]] <- levels(birthwt$physician_visits)
    RFfit.train$xlevels[["race"]] <- levels(birthwt$race)
    RFfit.train$xlevels[["smoke"]] <- levels(birthwt$smoke)
    RFfit.train$xlevels[["preterm_labours"]] <- levels(birthwt$preterm_labours )
    RFfit.train$xlevels[["hyper_tension"]] <- levels(birthwt$hyper_tension)
    RFfit.train$xlevels[["uterine_irritability"]] <- levels(birthwt$uterine_irritability)
    RFprob.test = predict(RFfit.train,testxy.out,type="response")
    predictvalid = rep(1,dim(testxy.out)[1]); 
    predictvalid[RFprob.test > 0.5] = 2
  }
  #relabel as original values, not factor levels
  predictvalid = predictvalid-1  
  
  allpredictedCV.out[groupj.out] = predictvalid

}

# the output shows the different models selected in the outer loop - purpose is only to observe processing

```

Logistic Model appears to be the best model based on the 10 fold double cross validation approach. 
```{r}

table(Prediction = allpredictedCV.out,Actual = birthwt$low)
##cbind(birthwt$low,allpredictedCV.out)
CV10.out = sum(birthwt$low!=allpredictedCV.out)/n.out
p.out = 1-CV10.out; p.out  


table(birthwt$low)/n.out
# so (cross-validated) proportion 0.6455 of correct classifications  is an improvement, 
table(allpredictedCV.out)/n.out
```

Finally we will fit the entire data to the selected model and will also tweak the threshold to 0.293 based on the ROC curve
```{r}
log2fit = glm(low ~ last_menstral_weight + race + smoke + preterm_labours + 
    hyper_tension, data= birthwt, family=binomial)
##table(log2fit$fitted.values>0.50, birthwt$low)

library(pROC)
myroc = roc(response=birthwt$low, predictor=log2fit$fitted.values)
plot.roc(myroc,print.thres='best')
myroc

table(Predicted = log2fit$fitted.values>0.293, Actual = birthwt$low) 
```

